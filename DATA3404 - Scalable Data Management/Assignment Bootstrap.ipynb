{"cells":[{"cell_type":"markdown","source":["# DATA3404 Big Data Assignment Bootstrap"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8b51ee16-8af5-4fbc-8046-2138d8ced271","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### 1. Preparing GoogleDriveDownloader.\nThe following defines a GoogleDriveDownloader for this notebook, The original version of this from pypi is broken due to a Google update. This pared-down version solves the issue for our use case."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"417dda1a-5a2d-48e0-94bb-df8dae96b16e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from __future__ import print_function\nimport requests\nimport zipfile\nimport warnings\nfrom sys import stdout\nfrom os import makedirs\nfrom os.path import dirname\nfrom os.path import exists\n\n\nclass GoogleDriveDownloader:\n    CHUNK_SIZE = 32768\n    DOWNLOAD_URL = 'https://docs.google.com/uc?export=download'\n\n    @staticmethod\n    def download_file_from_google_drive(file_id, dest_path, overwrite=False, unzip=False):\n        destination_directory = dirname(dest_path)\n        if not exists(destination_directory):\n            makedirs(destination_directory)\n\n        if not exists(dest_path) or overwrite:\n            session = requests.Session()\n            print('Downloading {} into {}... '.format(file_id, dest_path), end='')\n            stdout.flush()\n            params = {'id': file_id, 'confirm': \"T\"}\n            response = session.get(GoogleDriveDownloader.DOWNLOAD_URL, params=params, stream=True)\n            GoogleDriveDownloader._save_response_content(response, dest_path)\n            print('Done.')\n\n            if unzip:\n                try:\n                    print('Unzipping...', end='')\n                    stdout.flush()\n                    with zipfile.ZipFile(dest_path, 'r') as z:\n                        z.extractall(destination_directory)\n                    print('Done.')\n                except zipfile.BadZipfile:\n                    warnings.warn('Ignoring `unzip` since \"{}\" does not look like a valid zip file'.format(file_id))\n\n    @staticmethod\n    def _save_response_content(response, destination):\n        with open(destination, 'wb') as f:\n            for chunk in response.iter_content(GoogleDriveDownloader.CHUNK_SIZE):\n                if chunk:  # filter out keep-alive new chunks\n                    f.write(chunk)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4788dab7-bd6c-4694-b00d-c5180e7766b3","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 2. Download datafiles from Google Drive and upload them into Databricks FileStore.\nWe perform these steps initially so that the data persists. If we download them locally into the cluster, the data will not persist after the cluster has been terminated.\n\nFirst we define which files we want to retrieve from the lecture's Google Droive folder."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e2cea5a8-070c-43ee-8d59-000857fa7b85","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["dbfs_fileStore_prefix = \"/FileStore/tables\"\nprefix = \"ontimeperformance\"\nfiles = [\n  { \"name\": f\"{prefix}_flights_small.csv\",   \"file_id\": \"1MtLQ9e9rZkqV5LCZykrke0JFmJer38A5\" },\n  { \"name\": f\"{prefix}_flights_medium.csv\",  \"file_id\": \"1JhpTPr-CLeIKu-NyBALEwwYaUsjqIoU9\" },\n  { \"name\": f\"{prefix}_flights_large.csv\", \"file_id\": \"1lF4DL5U3gTKmbV4qFcHkev0sV1vP8gqw\" },\n  { \"name\": f\"{prefix}_airlines.csv\", \"file_id\": \"1hnKMGW2GTATRzn6T7ut9GgaJ7-d4RAH7\" },\n  { \"name\": f\"{prefix}_airports.csv\", \"file_id\": \"195HuQBaixRCyMrT-p6hv-GuZl7zsL5qI\" },\n  { \"name\": f\"{prefix}_aircrafts.csv\", \"file_id\": \"1wLFRY1Wi3knF-V9z_49zmxhmXhK-EiJe\" }\n]\n\nlarge_files =  { \"name\": f\"{prefix}_flights_large.zip\", \"file_id\": \"1USC4NI41j6LvqH7S4QYMn7wgHtWwSgo9\" }"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"81839ac7-f193-47b4-9a1b-d40215c6f9f7","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Preparation:** Remove existing files"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a3a9678d-8a99-4758-9b7f-b00a9430893e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import os\nfor file in files:\n  if os.path.exists(\"/tmp/{}\".format(file[\"name\"])):\n    os.remove(\"/tmp/{}\".format(file[\"name\"]))\n  dbutils.fs.rm(\"/FileStore/tables/{}\".format(file[\"name\"]))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9dca86a4-19e7-474a-ba96-f6c94aff1f6e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["**Download:** Download small, medium and large (1GB) datasets to your DBFS.\n\nIf this includes the large datafile, may take 20 to 30 seconds to complete, plus another minute or so for the subsquent conversion into the tables folder."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4b5ca2a6-2afe-46da-b9bd-0e45a3be5def","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["for file in files:\n  GoogleDriveDownloader.download_file_from_google_drive(file_id=file[\"file_id\"], dest_path=\"/tmp/{}\".format(file[\"name\"]))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"91453a50-c6d1-415a-9abe-dc36573b0a7d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["for file in files:\n  dbutils.fs.mv(\"file:/tmp/{}\".format(file[\"name\"]), \"/FileStore/tables/{}\".format(file[\"name\"]))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ac29e63f-7971-4750-bf80-561e85070ef4","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 3. Check the files in Databricks FileStore.\nYou can use the below cell to explore the DBFS directories and check that the files are where they should be, and contain the correct data."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"89d8d1ff-c732-4a27-85a9-ad1670d90537","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["display(dbutils.fs.ls(\"/FileStore/tables\"))\ndisplay(dbutils.fs.head(\"/FileStore/tables/ontimeperformance_flights_large.csv\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"555bd36a-e6f2-4274-b6a1-80bce462b806","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Congratulations, you have now all the datasets which you need for the Assignment 1 available.\n\nYou can now close this notebook and continue with the Demo notebook about how to access these datasets using SQL."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"60ebdd8f-1e4c-476e-a62d-ea6d4973c7da","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["# STOP HERE #\n\nThe next step will take several minutes and the massive dataset loaded there is only needed for the scalability evaluation at the end of your assignment. For starting on your assignment task, you are fine with the first three datasets loaded so far."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a08b98ad-d1f7-4d3f-954f-31668248db67","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Download extra large files (8 GB), extract, and move them to DBFS\n\nOnly do the next step once you need this massive 8GB dataset for the actual performance evaluation. It will take a long time..."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8fa7b842-6373-4305-9c8f-7521d92fd16a","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["\nif os.path.exists(\"/tmp/{}\".format(large_files[\"name\"])):\n  os.remove(\"/tmp/{}\".format(large_files[\"name\"]))\ndbutils.fs.rm(\"/FileStore/tables/{}\".format(f'{prefix}_flights_massive.csv'))\n  \nGoogleDriveDownloader.download_file_from_google_drive(file_id=large_files[\"file_id\"], \n                                    dest_path=\"/tmp/{}\".format(large_files[\"name\"]),\n                                    unzip=True)\n\n# dbutils.fs.mv(\"file:/tmp/8000mb_sample_flightdata.csv\", f\"{dbfs_fileStore_prefix}/{prefix}_flights_massive.csv\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b4ca8478-7808-4d2b-8122-189fca7ccd46","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dbutils.fs.mv(\"file:/tmp/ontimeperformance_flights_massive.csv\", f\"{dbfs_fileStore_prefix}/{prefix}_flights_massive.csv\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5dcb1ea1-b840-4fa0-b294-42aec98a8bba","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Assignment Bootstrap (1)","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3629711341512940}},"nbformat":4,"nbformat_minor":0}
